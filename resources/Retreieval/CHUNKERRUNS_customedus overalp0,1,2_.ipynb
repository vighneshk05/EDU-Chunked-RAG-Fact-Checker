{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dd8gcSCb20hu",
        "outputId": "49b446f3-166a-447a-cee1-7859ce3e1df0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'EDU-Chunking-RAG'...\n",
            "remote: Enumerating objects: 270, done.\u001b[K\n",
            "remote: Counting objects: 100% (270/270), done.\u001b[K\n",
            "remote: Compressing objects: 100% (189/189), done.\u001b[K\n",
            "remote: Total 270 (delta 125), reused 220 (delta 79), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (270/270), 494.88 KiB | 18.33 MiB/s, done.\n",
            "Resolving deltas: 100% (125/125), done.\n",
            "/content/EDU-Chunking-RAG\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/Arpnik/EDU-Chunking-RAG.git\n",
        "%cd EDU-Chunking-RAG\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L04jpOyZdyhQ",
        "outputId": "c7e5f64f-4412-4b6e-cbec-cc77951cee6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/EDU-Chunking-RAG\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.3\n",
            "Obtaining file:///content/EDU-Chunking-RAG\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch==2.9.0 in /usr/local/lib/python3.12/dist-packages (from fever-rag==0.1.0) (2.9.0+cu126)\n",
            "Requirement already satisfied: transformers==4.57.1 in /usr/local/lib/python3.12/dist-packages (from fever-rag==0.1.0) (4.57.1)\n",
            "Collecting qdrant-client==1.15.1 (from fever-rag==0.1.0)\n",
            "  Downloading qdrant_client-1.15.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: sentence-transformers>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from fever-rag==0.1.0) (5.1.2)\n",
            "Requirement already satisfied: scikit-learn>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from fever-rag==0.1.0) (1.6.1)\n",
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.12/dist-packages (from fever-rag==0.1.0) (4.67.1)\n",
            "Collecting peft==0.17.1 (from fever-rag==0.1.0)\n",
            "  Downloading peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from fever-rag==0.1.0) (2.32.4)\n",
            "Requirement already satisfied: numpy>=1.24.0 in /usr/local/lib/python3.12/dist-packages (from fever-rag==0.1.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.17.1->fever-rag==0.1.0) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft==0.17.1->fever-rag==0.1.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft==0.17.1->fever-rag==0.1.0) (6.0.3)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.17.1->fever-rag==0.1.0) (1.11.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft==0.17.1->fever-rag==0.1.0) (0.7.0)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.17.1->fever-rag==0.1.0) (0.36.0)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client==1.15.1->fever-rag==0.1.0) (1.76.0)\n",
            "Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.20.0->qdrant-client==1.15.1->fever-rag==0.1.0) (0.28.1)\n",
            "Collecting portalocker<4.0,>=2.7.0 (from qdrant-client==1.15.1->fever-rag==0.1.0)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client==1.15.1->fever-rag==0.1.0) (5.29.5)\n",
            "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in /usr/local/lib/python3.12/dist-packages (from qdrant-client==1.15.1->fever-rag==0.1.0) (2.11.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.12/dist-packages (from qdrant-client==1.15.1->fever-rag==0.1.0) (2.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->fever-rag==0.1.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->fever-rag==0.1.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->fever-rag==0.1.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->fever-rag==0.1.0) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->fever-rag==0.1.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->fever-rag==0.1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->fever-rag==0.1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->fever-rag==0.1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->fever-rag==0.1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->fever-rag==0.1.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->fever-rag==0.1.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->fever-rag==0.1.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->fever-rag==0.1.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->fever-rag==0.1.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->fever-rag==0.1.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->fever-rag==0.1.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->fever-rag==0.1.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->fever-rag==0.1.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->fever-rag==0.1.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->fever-rag==0.1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->fever-rag==0.1.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->fever-rag==0.1.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->fever-rag==0.1.0) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->fever-rag==0.1.0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->fever-rag==0.1.0) (0.22.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft==0.17.1->fever-rag==0.1.0) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client==1.15.1->fever-rag==0.1.0) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client==1.15.1->fever-rag==0.1.0) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client==1.15.1->fever-rag==0.1.0) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client==1.15.1->fever-rag==0.1.0) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client==1.15.1->fever-rag==0.1.0) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.20.0->qdrant-client==1.15.1->fever-rag==0.1.0) (4.3.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client==1.15.1->fever-rag==0.1.0) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client==1.15.1->fever-rag==0.1.0) (4.1.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client==1.15.1->fever-rag==0.1.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client==1.15.1->fever-rag==0.1.0) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client==1.15.1->fever-rag==0.1.0) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->fever-rag==0.1.0) (3.4.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.0->fever-rag==0.1.0) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.0->fever-rag==0.1.0) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.0->fever-rag==0.1.0) (3.6.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.2.0->fever-rag==0.1.0) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.0->fever-rag==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client==1.15.1->fever-rag==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.0->fever-rag==0.1.0) (3.0.3)\n",
            "Downloading peft-0.17.1-py3-none-any.whl (504 kB)\n",
            "Downloading qdrant_client-1.15.1-py3-none-any.whl (337 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Building wheels for collected packages: fever-rag\n",
            "  Building editable for fever-rag (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fever-rag: filename=fever_rag-0.1.0-0.editable-py3-none-any.whl size=5331 sha256=0596a340a107063ba99d14e9e5f0623f6cb1e0f7aba9392938d5581765d77506\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-d5o63h83/wheels/5e/26/ba/bb65e0c0db944c0f992ec6ba1c7944f620e2a5ab5b9fd574cb\n",
            "Successfully built fever-rag\n",
            "Installing collected packages: portalocker, qdrant-client, peft, fever-rag\n",
            "\u001b[2K  Attempting uninstall: peft\n",
            "\u001b[2K    Found existing installation: peft 0.18.0\n",
            "\u001b[2K    Uninstalling peft-0.18.0:\n",
            "\u001b[2K      Successfully uninstalled peft-0.18.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [fever-rag]\n",
            "\u001b[1A\u001b[2KSuccessfully installed fever-rag-0.1.0 peft-0.17.1 portalocker-3.2.0 qdrant-client-1.15.1\n"
          ]
        }
      ],
      "source": [
        "%cd /content/EDU-Chunking-RAG\n",
        "\n",
        "# Make sure pip is recent\n",
        "!pip install -U pip\n",
        "\n",
        "# Install this project in editable mode (uses pyproject.toml)\n",
        "!pip install -e .\n",
        "\n",
        "# (Optional) If you ever see \"ModuleNotFoundError\" for something, just pip install it:\n",
        "# !pip install qdrant-client sentence-transformers transformers tqdm torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "iuYMxw6edyjv",
        "outputId": "785422a1-341e-45e7-aa2d-2d3811bf3c22"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-320f520d-c778-4032-9f0b-c68934634293\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-320f520d-c778-4032-9f0b-c68934634293\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving reduced_fever_data.zip to reduced_fever_data.zip\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload your `fever_dataset.zip` from your machine\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f9nL9_odymL",
        "outputId": "10a2f64e-0c87-4821-e5bd-627fdca0d2e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/EDU-Chunking-RAG\n",
            "Archive:  reduced_fever_data.zip\n",
            "   creating: dataset/reduced_fever_data/\n",
            "  inflating: dataset/__MACOSX/._reduced_fever_data  \n",
            "  inflating: dataset/reduced_fever_data/train.jsonl  \n",
            "  inflating: dataset/__MACOSX/reduced_fever_data/._train.jsonl  \n",
            "  inflating: dataset/reduced_fever_data/paper_test.jsonl  \n",
            "  inflating: dataset/__MACOSX/reduced_fever_data/._paper_test.jsonl  \n",
            "   creating: dataset/reduced_fever_data/wiki/\n",
            "  inflating: dataset/__MACOSX/reduced_fever_data/._wiki  \n",
            "  inflating: dataset/reduced_fever_data/paper_dev.jsonl  \n",
            "  inflating: dataset/__MACOSX/reduced_fever_data/._paper_dev.jsonl  \n",
            "  inflating: dataset/reduced_fever_data/wiki/filtered_evidence.jsonl  \n",
            "  inflating: dataset/__MACOSX/reduced_fever_data/wiki/._filtered_evidence.jsonl  \n",
            "data_exploration.ipynb\t__MACOSX  reduced_fever_data\n",
            "paper_dev.jsonl  paper_test.jsonl  train.jsonl\twiki\n",
            "filtered_evidence.jsonl\n"
          ]
        }
      ],
      "source": [
        "%cd /content/EDU-Chunking-RAG\n",
        "\n",
        "# Just unzip INTO the existing dataset/ folder\n",
        "!unzip -o reduced_fever_data.zip -d dataset\n",
        "\n",
        "# Sanity checks\n",
        "!ls dataset\n",
        "!ls dataset/reduced_fever_data\n",
        "!ls dataset/reduced_fever_data/wiki\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "5Jh07gdkdyoV",
        "outputId": "d04649ef-5454-4013-9f51-94bd60f663e8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-60ffd467-1865-45eb-aa27-1bb34556e87d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-60ffd467-1865-45eb-aa27-1bb34556e87d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES.zip to MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES.zip\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # upload something like edu_segmenter_linear.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QVLw1Szd4j4",
        "outputId": "af992fb3-18db-47d9-9e17-d9152350af71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/EDU-Chunking-RAG\n",
            "Archive:  MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES.zip\n",
            "   creating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/\n",
            "   creating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/\n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/vocab.txt  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/adapter_config.json  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/rng_state.pth  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/adapter_model.safetensors  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/tokenizer_config.json  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/scaler.pt  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/special_tokens_map.json  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/trainer_state.json  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/training_args.bin  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/tokenizer.json  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/scheduler.pt  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/optimizer.pt  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/README.md  \n",
            "   creating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/best_model/\n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/best_model/vocab.txt  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/best_model/adapter_config.json  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/best_model/adapter_model.safetensors  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/best_model/tokenizer_config.json  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/best_model/special_tokens_map.json  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/best_model/training_args.bin  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/best_model/tokenizer.json  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/best_model/chunker_config.json  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/best_model/README.md  \n",
            "   creating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/\n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/vocab.txt  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/adapter_config.json  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/rng_state.pth  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/adapter_model.safetensors  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/tokenizer_config.json  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/scaler.pt  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/special_tokens_map.json  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/trainer_state.json  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/training_args.bin  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/tokenizer.json  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/scheduler.pt  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/optimizer.pt  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/README.md  \n",
            "   creating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/\n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/vocab.txt  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/adapter_config.json  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/rng_state.pth  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/adapter_model.safetensors  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/tokenizer_config.json  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/scaler.pt  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/special_tokens_map.json  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/trainer_state.json  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/training_args.bin  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/tokenizer.json  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/scheduler.pt  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/optimizer.pt  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/README.md  \n",
            "   creating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/\n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/vocab.txt  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/adapter_config.json  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/rng_state.pth  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/adapter_model.safetensors  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/tokenizer_config.json  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/scaler.pt  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/special_tokens_map.json  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/trainer_state.json  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/training_args.bin  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/tokenizer.json  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/scheduler.pt  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/optimizer.pt  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/README.md  \n",
            "   creating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/\n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/vocab.txt  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/adapter_config.json  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/rng_state.pth  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/adapter_model.safetensors  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/tokenizer_config.json  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/scaler.pt  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/special_tokens_map.json  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/trainer_state.json  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/training_args.bin  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/tokenizer.json  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/scheduler.pt  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/optimizer.pt  \n",
            "  inflating: edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/README.md  \n"
          ]
        }
      ],
      "source": [
        "%cd /content/EDU-Chunking-RAG\n",
        "!mkdir -p edu_segmenter_linear\n",
        "!unzip -o MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES.zip -d edu_segmenter_linear\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJJk3pPvd4mW",
        "outputId": "3eab59d9-54ed-4d15-95c7-333ba171b44d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES\n"
          ]
        }
      ],
      "source": [
        "!ls edu_segmenter_linear\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCnkwoovd4pA",
        "outputId": "3575c8c9-8f5d-4f74-cdb1-d7704b8a4301"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/EDU-Chunking-RAG\n",
            "best_model\tcheckpoint-546\tcheckpoint-702\n",
            "checkpoint-468\tcheckpoint-624\tcheckpoint-780\n"
          ]
        }
      ],
      "source": [
        "%cd /content/EDU-Chunking-RAG\n",
        "!ls edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GPTNszvd4rl"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = \"edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/best_model\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcvIk7MQdyqz",
        "outputId": "5ae76ecd-09d7-401e-f087-8fecd4e1cd7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATASET_ROOT: /content/EDU-Chunking-RAG/dataset/reduced_fever_data\n",
            "WIKI_DIR: /content/EDU-Chunking-RAG/dataset/reduced_fever_data/wiki\n",
            "CLAIM_FILE: /content/EDU-Chunking-RAG/dataset/reduced_fever_data/paper_dev.jsonl\n",
            "MODEL_PATH: /content/EDU-Chunking-RAG/edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/best_model\n",
            "/content/EDU-Chunking-RAG/dataset/reduced_fever_data exists: True\n",
            "/content/EDU-Chunking-RAG/dataset/reduced_fever_data/wiki exists: True\n",
            "/content/EDU-Chunking-RAG/dataset/reduced_fever_data/paper_dev.jsonl exists: True\n",
            "/content/EDU-Chunking-RAG/edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/best_model exists: True\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Project root in Colab\n",
        "PROJECT_ROOT = Path(\"/content/EDU-Chunking-RAG\")\n",
        "\n",
        "# Point to the reduced dataset inside the existing dataset/ folder\n",
        "DATASET_ROOT = PROJECT_ROOT / \"dataset\" / \"reduced_fever_data\"\n",
        "WIKI_DIR_PATH = DATASET_ROOT / \"wiki\"\n",
        "CLAIM_FILE_PATH = DATASET_ROOT / \"paper_dev.jsonl\"\n",
        "\n",
        "# EDU segmenter model path (adjust folder name if yours is different)\n",
        "MODEL_PATH_PATH = PROJECT_ROOT / \"edu_segmenter_linear/MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES\"  / \"best_model\"\n",
        "\n",
        "# String versions for use in !python CLI commands\n",
        "DATASET_ROOT_STR = str(DATASET_ROOT)\n",
        "WIKI_DIR = str(WIKI_DIR_PATH)\n",
        "CLAIM_FILE = str(CLAIM_FILE_PATH)\n",
        "MODEL_PATH = str(MODEL_PATH_PATH)\n",
        "\n",
        "print(\"DATASET_ROOT:\", DATASET_ROOT_STR)\n",
        "print(\"WIKI_DIR:\", WIKI_DIR)\n",
        "print(\"CLAIM_FILE:\", CLAIM_FILE)\n",
        "print(\"MODEL_PATH:\", MODEL_PATH)\n",
        "\n",
        "# Optional sanity check: see which ones actually exist\n",
        "for p in [DATASET_ROOT, WIKI_DIR_PATH, CLAIM_FILE_PATH, MODEL_PATH_PATH]:\n",
        "    print(p, \"exists:\", p.exists())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XX3D9kfbdytN",
        "outputId": "ceaad9ce-f3f2-4678-a6de-8565b3bc18ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patched data_helper.py ✅\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "file_path = Path(\"/content/EDU-Chunking-RAG/com/fever/rag/utils/data_helper.py\")\n",
        "text = file_path.read_text()\n",
        "\n",
        "# Replace the wrong import with the correct one\n",
        "text = text.replace(\n",
        "    \"from sympy.printing.pytorch import torch\",\n",
        "    \"import torch\"\n",
        ")\n",
        "\n",
        "file_path.write_text(text)\n",
        "print(\"Patched data_helper.py ✅\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HXelW_8d-tS",
        "outputId": "2ce51427-4dcd-436f-908e-19ad845a4b64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/EDU-Chunking-RAG\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: qdrant-client in /usr/local/lib/python3.12/dist-packages (1.15.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (1.76.0)\n",
            "Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.28.1)\n",
            "Requirement already satisfied: portalocker<4.0,>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (3.2.0)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (5.29.5)\n",
            "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (2.11.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (2.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.3.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "%cd /content/EDU-Chunking-RAG\n",
        "!pip install sentence-transformers qdrant-client\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwSWF5moeiE_",
        "outputId": "093d4fb5-de1d-4b7b-ad69-264f24055a08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.path.exists(MODEL_PATH))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# 1. Mount Google Drive (you'll do the auth once per runtime)\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vx-UE--iPaM2",
        "outputId": "bf0cc92b-ca02-42b8-a4d4-850f1042bfeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(os.path.ismount(\"/content/drive\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59m83uYXPaZW",
        "outputId": "09d7b629-756e-4bc3-edd7-260d97400046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdlR-qwcPuVH",
        "outputId": "d7cfecde-5faf-4d78-cd47-a397cc4ae344"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biQNlgI5Pud1",
        "outputId": "f7c25e2c-29d0-4f94-ea30-64d0d9f7e9f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Colab Notebooks'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/EDU-Chunking-RAG\n",
        "\n",
        "# Run retriever with sentence chunker + threshold strategy\n",
        "!python -m com.fever.rag.retriever.retriever_evaluator \\\n",
        "  --qdrant_in_memory True \\\n",
        "  --embedding_model_name sentence-transformers/all-MiniLM-L6-v2 \\\n",
        "  --wiki_dir dataset/reduced_fever_data/wiki \\\n",
        "  --output_file retrieval_evaluation_results_sentence_threshold0_8.jsonl \\\n",
        "  --claim_file_path dataset/reduced_fever_data/paper_dev.jsonl \\\n",
        "  --chunker_type sentence \\\n",
        "  --retrieval_strategy threshold \\\n",
        "  --max_tokens 150 \\\n",
        "  --threshold 0.8 > sentence_threshold0_8.log 2>&1\n",
        "\n",
        "# Copy result + log to Google Drive (MyDrive root)\n",
        "!cp retrieval_evaluation_results_sentence_threshold0_8.jsonl /content/drive/MyDrive/\n",
        "!cp sentence_threshold0_8.log /content/drive/MyDrive/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yn3bzHVJQ1P5",
        "outputId": "424df14c-2b54-4bad-e164-db4aaa1a63d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/EDU-Chunking-RAG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- overlap = 2 ----------\n",
        "!python -m com.fever.rag.retriever.retriever_evaluator \\\n",
        "  --qdrant_in_memory True \\\n",
        "  --embedding_model_name sentence-transformers/all-MiniLM-L6-v2 \\\n",
        "  --wiki_dir dataset/reduced_fever_data/wiki \\\n",
        "  --output_file retrieval_evaluation_results_overlap2.jsonl \\\n",
        "  --claim_file_path dataset/reduced_fever_data/paper_dev.jsonl \\\n",
        "  --chunker_type custom_edu \\\n",
        "  --max_tokens 150 \\\n",
        "  --chunking_overlap 2 \\\n",
        "  --retrieval_strategy top_k \\\n",
        "  --model_path \"$MODEL_PATH\" \\\n",
        "  --top_k 20 > overlap2.log 2>&1\n",
        "\n",
        "!cp retrieval_evaluation_results_overlap2.jsonl /content/drive/MyDrive/\n",
        "!cp overlap2.log /content/drive/MyDrive/\n"
      ],
      "metadata": {
        "id": "s7CC89wnWsNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- overlap = 1 ----------\n",
        "!python -m com.fever.rag.retriever.retriever_evaluator \\\n",
        "  --qdrant_in_memory True \\\n",
        "  --embedding_model_name sentence-transformers/all-MiniLM-L6-v2 \\\n",
        "  --wiki_dir dataset/reduced_fever_data/wiki \\\n",
        "  --output_file retrieval_evaluation_results_overlap1.jsonl \\\n",
        "  --claim_file_path dataset/reduced_fever_data/paper_dev.jsonl \\\n",
        "  --chunker_type custom_edu \\\n",
        "  --max_tokens 150 \\\n",
        "  --chunking_overlap 1 \\\n",
        "  --retrieval_strategy top_k \\\n",
        "  --model_path \"$MODEL_PATH\" \\\n",
        "  --top_k 20 > overlap1.log 2>&1\n",
        "\n",
        "!cp retrieval_evaluation_results_overlap1.jsonl /content/drive/MyDrive/\n",
        "!cp overlap1.log /content/drive/MyDrive/\n"
      ],
      "metadata": {
        "id": "GAbWg4vnWley"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/EDU-Chunking-RAG\n",
        "\n",
        "# ---------- overlap = 0 ----------\n",
        "!python -m com.fever.rag.retriever.retriever_evaluator \\\n",
        "  --qdrant_in_memory True \\\n",
        "  --embedding_model_name sentence-transformers/all-MiniLM-L6-v2 \\\n",
        "  --wiki_dir dataset/reduced_fever_data/wiki \\\n",
        "  --output_file retrieval_evaluation_results_overlap0.jsonl \\\n",
        "  --claim_file_path dataset/reduced_fever_data/paper_dev.jsonl \\\n",
        "  --chunker_type custom_edu \\\n",
        "  --max_tokens 150 \\\n",
        "  --chunking_overlap 0 \\\n",
        "  --retrieval_strategy top_k \\\n",
        "  --model_path \"$MODEL_PATH\" \\\n",
        "  --top_k 20 > overlap0.log 2>&1\n",
        "\n",
        "!cp retrieval_evaluation_results_overlap0.jsonl /content/drive/MyDrive/\n",
        "!cp overlap0.log /content/drive/MyDrive/\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Z2bLt9B8zLH",
        "outputId": "f34d7385-988f-45fd-acaa-ddd83b64a53d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/EDU-Chunking-RAG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/EDU-Chunking-RAG\n",
        "\n",
        "# Run retriever with sentence chunker + threshold strategy\n",
        "!python -m com.fever.rag.retriever.retriever_evaluator \\\n",
        "  --qdrant_in_memory True \\\n",
        "  --embedding_model_name sentence-transformers/all-MiniLM-L6-v2 \\\n",
        "  --wiki_dir dataset/reduced_fever_data/wiki \\\n",
        "  --output_file retrieval_evaluation_results_sentence_threshold0_8.jsonl \\\n",
        "  --claim_file_path dataset/reduced_fever_data/paper_dev.jsonl \\\n",
        "  --chunker_type sentence \\\n",
        "  --retrieval_strategy threshold \\\n",
        "  --max_tokens 150 \\\n",
        "  --threshold 0.6 > sentence_threshold0_6.log 2>&1\n",
        "\n",
        "# Copy result + log to Google Drive (MyDrive root)\n",
        "!cp retrieval_evaluation_results_sentence_threshold0_6.jsonl /content/drive/MyDrive/\n",
        "!cp sentence_threshold0_6.log /content/drive/MyDrive/\n"
      ],
      "metadata": {
        "id": "dYNGOEefI-JR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae9564e2-cb89-4816-c938-3b094b0dea12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/EDU-Chunking-RAG\n",
            "cp: cannot stat 'retrieval_evaluation_results_sentence_threshold0_6.jsonl': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2wxaifxmcPYU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}